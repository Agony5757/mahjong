{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "memories = [MahjongBufferFrost2(size=4096, num_tile_type=num_tile_type, num_each_tile=num_each_tile,\n",
    "                                num_vf=num_vf) for i in range(4)]\n",
    "\n",
    "episode_start = 256\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40\n",
    "max_steps = memories[0].episode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 44 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 135 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 192 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 270 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 289 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 342 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 366 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 380 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 401 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 409 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 509 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 610 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 648 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 801 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 826 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 1063 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1201 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1224 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1432 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 1634 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1654 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1714 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1767 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1812 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1826 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1871 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1950 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 1952 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1963 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 1978 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 2034 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 2193 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 2198 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 2240 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 2435 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 2452 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 2622 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 2874 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 2930 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 2943 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 2970 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3023 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 3121 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3153 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3238 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3243 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 3254 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3273 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3368 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3388 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3425 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3452 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 3544 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3568 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 3622 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3670 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3671 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 3726 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3760 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3873 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 3886 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 3991 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 4003 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 4127 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 4276 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 4421 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 4490 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 4498 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 4521 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 4603 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 4764 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 4815 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 4841 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 4906 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 5113 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 5186 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 5201 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 5273 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 5308 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 5600 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 5608 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 5786 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 5789 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 5880 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 5882 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 5990 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6005 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6079 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6109 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6110 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6140 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6161 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6178 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6188 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6209 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6223 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6300 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6357 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6362 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6413 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6423 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6435 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6445 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6446 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6465 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6548 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6668 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 6672 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6688 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6707 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6812 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6878 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6884 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 6903 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6908 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 6991 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7031 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 7050 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 7171 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 7267 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7533 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 7564 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7579 \n",
      "ResultType.NoTileRyuuKyoku: Totally 150 steps\n",
      " Game 7636 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 7637 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7654 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 7657 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7702 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      "Buffer saved in path: ./buffer/Agent0-MahjongBufferFrost2_RanHoraPolicy_20190601-173037.pkl\n",
      "Buffer saved in path: ./buffer/Agent1-MahjongBufferFrost2_RanHoraPolicy_20190601-173037.pkl\n",
      "Buffer saved in path: ./buffer/Agent2-MahjongBufferFrost2_RanHoraPolicy_20190601-173037.pkl\n",
      "Buffer saved in path: ./buffer/Agent3-MahjongBufferFrost2_RanHoraPolicy_20190601-173037.pkl\n",
      " Game 7775 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7843 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7954 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 7959 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 7987"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "FILE:..\\..\\Mahjong\\ScoreCounter.cpp LINE:408 FUNC:CounterResult::calculate_score MSG:ss.str().c_str()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-810c3b608324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[0met\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[0mselect_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0met\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayerNo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[1;31m## Note: next_states is agent's prediction, but not the true one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Codes\\mahjong\\AI\\wrapper.py\u001b[0m in \u001b[0;36mstep_response\u001b[1;34m(self, action, playerNo)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhoras\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayerNo\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayed_a_tile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayerNo\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: FILE:..\\..\\Mahjong\\ScoreCounter.cpp LINE:408 FUNC:CounterResult::calculate_score MSG:ss.str().c_str()"
     ]
    }
   ],
   "source": [
    "n_games = 100000\n",
    "\n",
    "process_time = 0\n",
    "learn_time = 0 \n",
    "select_time = 0\n",
    "all_time = 0\n",
    "play_time = 0\n",
    "response_time = 0\n",
    "copy_time = 0\n",
    "done_time = 0\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    st_all =  time.time()\n",
    "    \n",
    "#     if n % 10000 == 0:\n",
    "#         for i in range(4):\n",
    "#             agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    \n",
    "    for i in range(4):\n",
    "        if memories[i].tail % episode_savebuffer == 0:\n",
    "            memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_\" + datetime_str + \".pkl\")\n",
    "    \n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = np.zeros([4, max_steps], dtype=np.float16)\n",
    "    episode_matrix_features = np.zeros([4, max_steps, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    episode_vector_features = np.zeros([4, max_steps, num_vf], dtype=np.float16)\n",
    "    episode_rewards = np.zeros([4, max_steps], dtype=np.float32)\n",
    "    episode_actions = np.zeros([4, max_steps], dtype=np.int32)\n",
    "    episode_policies = np.zeros([4, max_steps, mu_size], dtype=np.float32)\n",
    "\n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    agent_step = [0, 0, 0, 0]\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        st_play = time.time()\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = 0\n",
    "            policy = np.zeros([mu_size, ], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who, agent_step[who]] = policy  # only 1 available action (draw)\n",
    "\n",
    "            agent_step[who] += 1\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            \n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "\n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            #######################################\n",
    "#             st_process = time.time()\n",
    "\n",
    "#             next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "#             et_process = time.time()\n",
    "#             process_time += et_process - st_process     \n",
    "            \n",
    "            st = time.time()\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                \n",
    "                a_in_good_as = np.random.choice(len(good_actions))\n",
    "                policy = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                \n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action = np.random.choice(len(aval_actions))\n",
    "                policy = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            \n",
    "            et = time.time()\n",
    "            select_time += et - st\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = action\n",
    "            episode_policies[who, agent_step[who]] = policy\n",
    "            agent_step[who] += 1\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            et_play = time.time()\n",
    "            play_time += et_play - st_play\n",
    "#             step += 2\n",
    "        \n",
    "        st_response = time.time()\n",
    "        if what == \"response\":\n",
    "            policies = [np.zeros([mu_size,], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                \n",
    "#                 st_process = time.time()\n",
    "#                 next_aval_states = env.get_aval_next_states(i)\n",
    "#                 et_process = time.time()\n",
    "#                 process_time += et_process - st_process\n",
    "                \n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                \n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                st = time.time()\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as = np.random.choice(len(good_actions))\n",
    "                    policies[i] = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                    \n",
    "                else:\n",
    "                    actions[i] = np.random.choice(len(aval_actions))\n",
    "                    policies[i] = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                \n",
    "                et = time.time()\n",
    "                select_time += et - st\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                episode_dones[i, agent_step[i]] = done\n",
    "                episode_matrix_features[i, agent_step[i]] = this_states[i][0]\n",
    "                episode_vector_features[i, agent_step[i]] = this_states[i][1]\n",
    "                episode_rewards[i, agent_step[i]] = rs[i]\n",
    "                episode_actions[i, agent_step[i]] = actions[i]\n",
    "                episode_policies[i, agent_step[i]] = policies[i]\n",
    "                agent_step[i] += 1\n",
    "                \n",
    "            ## next step\n",
    "            st_copy = time.time()\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            et_copy = time.time()\n",
    "            copy_time += et_copy - st_copy\n",
    "            \n",
    "            step += 1\n",
    "        et_response = time.time()\n",
    "        response_time += et_response - st_response\n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            st_done = time.time()\n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                current_state = env.get_state_(i)\n",
    "                episode_matrix_features[i, agent_step[i]] = current_state[0]\n",
    "                episode_vector_features[i, agent_step[i]] = current_state[1]\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    memories[i].append_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                               episode_vector_features[i, 0: agent_step[i]],\n",
    "                                               episode_rewards[i, 0: agent_step[i]],\n",
    "                                               episode_dones[i, 0: agent_step[i]],\n",
    "                                               episode_actions[i, 0: agent_step[i]],\n",
    "                                               episode_policies[i, 0: agent_step[i]],\n",
    "                                               weight=0)\n",
    "#                     agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "#                                                episode_dones[i], episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "#                 with open(\"./Paipu/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".txt\", 'w+') as fp:\n",
    "#                     fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "#                     break\n",
    "            else:\n",
    "                if np.random.rand() < 0.005: ## no score change\n",
    "                    for i in range(4):\n",
    "                        memories[i].append_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                                   episode_vector_features[i, 0: agent_step[i]],\n",
    "                                                   episode_rewards[i, 0: agent_step[i]],\n",
    "                                                   episode_dones[i, 0: agent_step[i]],\n",
    "                                                   episode_actions[i, 0: agent_step[i]],\n",
    "                                                   episode_policies[i, 0: agent_step[i]],\n",
    "                                                   weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "            \n",
    "#             st = time.time()\n",
    "#             for n_train in range(5):\n",
    "#                 for i in range(4):\n",
    "#                     agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start, logging=True)\n",
    "#             et = time.time()\n",
    "#             learn2_time += et - st\n",
    "            \n",
    "#             et_done = time.time()\n",
    "#             done_time += et_done - st_done\n",
    "            \n",
    "            et_all = time.time()\n",
    "            all_time += et_all - st_all\n",
    "\n",
    "# data = {\"rons\": env.final_score_changes}\n",
    "# sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories[1].filled_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_228\"  + \".npz\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 0\n",
      "BaseTile._2m\n",
      "BaseTile._9m\n",
      "BaseTile._9m\n",
      "BaseTile._1s\n",
      "BaseTile._6s\n",
      "BaseTile._9s\n",
      "BaseTile._5p\n",
      "BaseTile._5p\n",
      "BaseTile._9p\n",
      "BaseTile.west\n",
      "player 1\n",
      "BaseTile._2m\n",
      "BaseTile._8m\n",
      "BaseTile._9m\n",
      "BaseTile._1s\n",
      "BaseTile._1s\n",
      "BaseTile._3s\n",
      "BaseTile._2p\n",
      "BaseTile._2p\n",
      "BaseTile._3p\n",
      "BaseTile.hatsu\n",
      "player 2\n",
      "BaseTile._3s\n",
      "BaseTile._3s\n",
      "BaseTile._6p\n",
      "BaseTile._8p\n",
      "player 3\n",
      "BaseTile._1m\n",
      "BaseTile._2m\n",
      "BaseTile._3m\n",
      "BaseTile._5m\n",
      "BaseTile._9m\n",
      "BaseTile._5s\n",
      "BaseTile.east\n",
      "BaseTile._7p\n",
      "player 0\n",
      "[1p][2p]([3p])\n",
      "player 1\n",
      "[8s]([8s])[8s]\n",
      "player 2\n",
      "[1p][2p]([3p])\n",
      "([7s])[8s][9s]\n",
      "[4m]([4m])[4m]\n",
      "player 3\n",
      "([4p])[5p][6p]\n",
      "[7p]([7p])[7p]\n"
     ]
    }
   ],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "FILE:..\\..\\Mahjong\\GameLog.cpp LINE:58 FUNC:BaseGameLog::to_string MSG:\"Invalid LogAction. Action: \" + std::to_string(int(action))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f3049fcec522>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Paipu/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"bug_paipu\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGameLogToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GBK'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: FILE:..\\..\\Mahjong\\GameLog.cpp LINE:58 FUNC:BaseGameLog::to_string MSG:\"Invalid LogAction. Action: \" + std::to_string(int(action))"
     ]
    }
   ],
   "source": [
    "with open(\"./Paipu/\"+\"bug_paipu\"+\".txt\", 'w+') as fp:\n",
    "    fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.GameLogToString(env.t.game_log).decode('GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.get_self_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
