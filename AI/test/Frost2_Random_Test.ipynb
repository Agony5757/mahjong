{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "memories = [MahjongBufferFrost2(size=1024, num_tile_type=num_tile_type, num_each_tile=num_each_tile,\n",
    "                                num_vf=num_vf) for i in range(4)]\n",
    "\n",
    "\n",
    "episode_start = 256\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dict = {\"S\":memories[1].S, \"s\":memories[1].s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./test.pkl\", 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./test.pkl\", 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost220190521-155324.npz\"\n",
    "#     agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     model_path =  \"../log/Agent{}\".foramt(i) + \"-20190501-175203-Game0/naiveAI.ckpt\"\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_action = mp.Action.Chi\n",
    "Phases = (\"P1_ACTION\", \"P2_ACTION\", \"P3_ACTION\", \"P4_ACTION\", \"P1_RESPONSE\", \"P2_RESPONSE\", \"P3_RESPONSE\",\n",
    "            \"P4_RESPONSE\", \"P1_抢杠RESPONSE\", \"P2_抢杠RESPONSE\", \"P3_抢杠RESPONSE\", \"P4_抢杠RESPONSE\",\n",
    "            \"P1_抢暗杠RESPONSE\", \"P2_抢暗杠RESPONSE\", \" P3_抢暗杠RESPONSE\", \" P4_抢暗杠RESPONSE\", \"GAME_OVER\",\n",
    "            \"P1_DRAW, P2_DRAW, P3_DRAW, P4_DRAW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 0----- Player 1 Chi -----\n",
      "BaseTile._1m\n",
      "BaseTile._2m\n",
      "BaseTile._3m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f77911fecf9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# table change after all players making actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_judge\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mfulus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchiplayerNo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfulus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----- player {} fulu ------'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchiplayerNo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unf' is not defined"
     ]
    }
   ],
   "source": [
    "stop_judge = 0\n",
    "n_games = 1000000\n",
    "\n",
    "\n",
    "process_time = 0\n",
    "learn_time = 0 \n",
    "select_time = 0\n",
    "all_time = 0\n",
    "play_time = 0\n",
    "response_time = 0\n",
    "copy_time = 0\n",
    "done_time = 0\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    st_all =  time.time()\n",
    "    \n",
    "#     if n % 10000 == 0:\n",
    "#         for i in range(4):\n",
    "#             agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if memories[i].tail % episode_savebuffer == 0:\n",
    "#             memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_\" + datetime_str + \".npz\")\n",
    "    \n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = [[], [], [], []]\n",
    "    episode_states = [[], [], [], []]\n",
    "    episode_rewards = [[], [], [], []]\n",
    "    episode_actions = [[], [], [], []]\n",
    "    episode_policies = [[], [], [], []]\n",
    "    \n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        st_play = time.time()\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_actions[who].append(0)\n",
    "            policy = np.zeros([mu_size,], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            \n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "\n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            #######################################\n",
    "#             st_process = time.time()\n",
    "\n",
    "#             next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "#             et_process = time.time()\n",
    "#             process_time += et_process - st_process     \n",
    "            \n",
    "            st = time.time()\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                \n",
    "                a_in_good_as = np.random.choice(len(good_actions))\n",
    "                policy = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                \n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action = np.random.choice(len(aval_actions))\n",
    "                policy = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            \n",
    "            et = time.time()\n",
    "            select_time += et - st\n",
    "            \n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "                \n",
    "            if aval_actions[actions[who]].action == stop_action:\n",
    "                print(\"stop, player{}\".format(who))\n",
    "                print(undefined)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_actions[who].append(action)\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            et_play = time.time()\n",
    "            play_time += et_play - st_play\n",
    "#             step += 2\n",
    "        \n",
    "        st_response = time.time()\n",
    "        if what == \"response\":\n",
    "            policies = [np.zeros([mu_size,], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "#                 st_process = time.time()\n",
    "#                 next_aval_states = env.get_aval_next_states(i)\n",
    "#                 et_process = time.time()\n",
    "#                 process_time += et_process - st_process\n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                \n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                st = time.time()\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as = np.random.choice(len(good_actions))\n",
    "                    policies[i] = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                    \n",
    "                else:\n",
    "                    actions[i] = np.random.choice(len(aval_actions))\n",
    "                    policies[i] = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                \n",
    "                et = time.time()\n",
    "                select_time += et - st\n",
    "#                 print(Phases[env.t.get_phase()])\n",
    "\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "    \n",
    "                ######## stop test #############\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                if aval_actions[actions[i]].action == stop_action:\n",
    "                    chiplayerNo = i\n",
    "                    stop_judge = 1\n",
    "                    print('----- Player {} Chi -----'.format(chiplayerNo))\n",
    "                    \n",
    "                    for k in range(len(aval_actions[actions[i]].correspond_tiles)):\n",
    "                        print(aval_actions[actions[i]].correspond_tiles[k].tile)\n",
    "                    print(env.t.get_selected_action_tile().tile)\n",
    "                    \n",
    "                    \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "            \n",
    "            # table change after all players making actions\n",
    "            if stop_judge:\n",
    "                print(unf)\n",
    "                fulus = env.t.players[chiplayerNo].fulus\n",
    "                print('----- player {} fulu ------'.format(chiplayerNo))\n",
    "                for k in range(len(fulus)):\n",
    "                    for m in range(len(fulus[k].tiles)):\n",
    "                        print(fulus[k].tiles[m].tile)\n",
    "                print('==========================='.format(chiplayerNo))\n",
    "                stop_judge = 0\n",
    "            \n",
    "            \n",
    "            for i in range(4):\n",
    "                next_states[i] = env.get_state_(i)\n",
    "                episode_dones[i].append(done)\n",
    "                episode_states[i].append(this_states[i])\n",
    "                episode_rewards[i].append(rs[i])\n",
    "                episode_actions[i].append(actions[i])\n",
    "                episode_policies[i].append(policies[i]) # only 1 available action (draw)\n",
    "        \n",
    "            ## next step\n",
    "            st_copy = time.time()\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            et_copy = time.time()\n",
    "            copy_time += et_copy - st_copy\n",
    "            \n",
    "            step += 1\n",
    "        et_response = time.time()\n",
    "        response_time += et_response - st_response\n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            st_done = time.time()\n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                episode_states[i].append(env.get_state_(i))\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    memories[i].append_episode(episode_states[i],\n",
    "                                               np.reshape(episode_rewards[i], [-1,]),\n",
    "                                               np.reshape(episode_dones[i], [-1,]),\n",
    "                                               np.reshape(episode_actions[i], [-1,]),\n",
    "                                               np.reshape(episode_policies[i], [-1, 40]),\n",
    "                                               weight=0)\n",
    "#                     agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "#                                                episode_dones[i], episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "#                 with open(\"./Paipu/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".txt\", 'w+') as fp:\n",
    "#                     fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "#                     break\n",
    "            else:\n",
    "                if np.random.rand() < 0.0005: ## no score change\n",
    "                    for i in range(4):\n",
    "                        memories[i].append_episode(episode_states[i],\n",
    "                                                   np.reshape(episode_rewards[i], [-1,]),\n",
    "                                                   np.reshape(episode_dones[i], [-1,]),\n",
    "                                                   np.reshape(episode_actions[i], [-1,]),\n",
    "                                                   np.reshape(episode_policies[i], [-1, 40]),\n",
    "                                                   weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "            \n",
    "#             st = time.time()\n",
    "#             for n_train in range(5):\n",
    "#                 for i in range(4):\n",
    "#                     agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start, logging=True)\n",
    "#             et = time.time()\n",
    "#             learn2_time += et - st\n",
    "            \n",
    "#             et_done = time.time()\n",
    "#             done_time += et_done - st_done\n",
    "            \n",
    "#             et_all = time.time()\n",
    "#             all_time += et_all - st_all\n",
    "\n",
    "# data = {\"rons\": env.final_score_changes}\n",
    "# sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_228\"  + \".npz\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.002000570297241211\n",
      "0\n",
      "0.019994735717773438\n",
      "0.0989370346069336\n",
      "0\n",
      "0\n",
      "0.0009999275207519531\n"
     ]
    }
   ],
   "source": [
    "print(all_time)\n",
    "print(select_time)\n",
    "print(learn_time)\n",
    "print(play_time)\n",
    "print(response_time)\n",
    "print(process_time)\n",
    "print(done_time)\n",
    "print(copy_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- player 0 hand -----\n",
      "BaseTile._2m\n",
      "BaseTile._9m\n",
      "BaseTile._1s\n",
      "BaseTile._3s\n",
      "BaseTile._4s\n",
      "BaseTile._5s\n",
      "BaseTile._3p\n",
      "BaseTile._4p\n",
      "BaseTile._4p\n",
      "BaseTile._5p\n",
      "BaseTile._7p\n",
      "BaseTile.west\n",
      "BaseTile.west\n",
      "----- player 1 hand -----\n",
      "BaseTile._1m\n",
      "BaseTile._7m\n",
      "BaseTile._7s\n",
      "BaseTile._7s\n",
      "BaseTile._8s\n",
      "BaseTile._1p\n",
      "BaseTile._2p\n",
      "BaseTile._2p\n",
      "BaseTile._5p\n",
      "BaseTile._6p\n",
      "BaseTile._9p\n",
      "----- player 2 hand -----\n",
      "BaseTile._3m\n",
      "BaseTile._4m\n",
      "BaseTile._8m\n",
      "BaseTile._8m\n",
      "BaseTile._6s\n",
      "BaseTile._6s\n",
      "BaseTile._8s\n",
      "BaseTile._2p\n",
      "BaseTile._5p\n",
      "BaseTile.east\n",
      "BaseTile.east\n",
      "BaseTile.south\n",
      "BaseTile.hatsu\n",
      "----- player 3 hand -----\n",
      "BaseTile._2m\n",
      "BaseTile._6m\n",
      "BaseTile._6m\n",
      "BaseTile._8m\n",
      "BaseTile._1s\n",
      "BaseTile._5s\n",
      "BaseTile._6s\n",
      "BaseTile._9s\n",
      "BaseTile._2p\n",
      "BaseTile.haku\n",
      "----- player 0 fulu -----\n",
      "----- player 1 fulu -----\n",
      "BaseTile._1m\n",
      "BaseTile._2m\n",
      "BaseTile._3m\n",
      "----- player 2 fulu -----\n",
      "----- player 3 fulu -----\n",
      "BaseTile.chu\n",
      "BaseTile.chu\n",
      "BaseTile.chu\n"
     ]
    }
   ],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('----- player {} hand -----'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('----- player {} fulu -----'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        for m in range(len(fulus[k].tiles)):\n",
    "            print(fulus[k].tiles[m].tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.get_selected_action_tile().tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "playerNo = 1\n",
    "\n",
    "plt.pcolor(env.get_state_(playerNo)[0][:,-29:])\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action.Play\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "playerNo = 0\n",
    "a = 10\n",
    "plt.pcolor(env.get_next_state(a, playerNo)[0][:,-5:])\n",
    "print(env.t.get_self_actions()[a].action)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.GameLogToString(env.t.game_log).decode('GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.get_self_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
