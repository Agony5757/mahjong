{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dongqi Han\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      memory=MahjongBufferFrost2(size=1024, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-1, 1), lambd=0.0,\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n",
    "\n",
    "\n",
    "episode_start = 256\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "for i in range(4):\n",
    "    buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost2_RanHoraPolicy_1024.npz\"\n",
    "    agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     model_path =  \"../log/Agent{}\".foramt(i) + \"-20190501-175203-Game0/naiveAI.ckpt\"\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Model saved in path: ../log/Agent0-20190528-130206-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent1-20190528-130206-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent2-20190528-130206-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent3-20190528-130206-Game0/naiveAI.ckpt\n",
      "Buffer saved in path: ./buffer/Agent0-MahjongBufferFrost220190528-130206.npz\n",
      "Buffer saved in path: ./buffer/Agent1-MahjongBufferFrost220190528-130206.npz\n",
      "Buffer saved in path: ./buffer/Agent2-MahjongBufferFrost220190528-130206.npz\n",
      "Buffer saved in path: ./buffer/Agent3-MahjongBufferFrost220190528-130206.npz\n",
      " Game 302 \n",
      "ResultType.RonAgari: Totally 36 steps\n",
      " Game 531 \n",
      "ResultType.TsumoAgari: Totally 65 steps\n",
      " Game 691 \n",
      "ResultType.RonAgari: Totally 102 steps\n",
      " Game 705 \n",
      "ResultType.NoTileRyuuKyoku: Totally 112 steps\n",
      " Game 872 \n",
      "ResultType.RonAgari: Totally 50 steps\n",
      " Game 928 \n",
      "ResultType.RonAgari: Totally 102 steps\n",
      " Game 1036 \n",
      "ResultType.NoTileRyuuKyoku: Totally 117 steps\n",
      " Game 1146 \n",
      "ResultType.NoTileRyuuKyoku: Totally 116 steps\n",
      " Game 1186 \n",
      "ResultType.RonAgari: Totally 114 steps\n",
      " Game 1301 \n",
      "ResultType.RonAgari: Totally 109 steps\n",
      " Game 1653 \n",
      "ResultType.NoTileRyuuKyoku: Totally 115 steps\n",
      " Game 1664 \n",
      "ResultType.NoTileRyuuKyoku: Totally 115 steps\n",
      " Game 1834 \n",
      "ResultType.NoTileRyuuKyoku: Totally 121 steps\n",
      " Game 2074 \n",
      "ResultType.NoTileRyuuKyoku: Totally 109 steps\n",
      " Game 2098 \n",
      "ResultType.NoTileRyuuKyoku: Totally 116 steps\n",
      " Game 2115 \n",
      "ResultType.RonAgari: Totally 96 steps\n",
      " Game 2123 \n",
      "ResultType.NoTileRyuuKyoku: Totally 110 steps\n",
      " Game 2165 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 2200 \n",
      "ResultType.NoTileRyuuKyoku: Totally 110 steps\n",
      " Game 2231 \n",
      "ResultType.NoTileRyuuKyoku: Totally 114 steps\n",
      " Game 2279 \n",
      "ResultType.TsumoAgari: Totally 118 steps\n",
      " Game 2304 \n",
      "ResultType.TsumoAgari: Totally 79 steps\n",
      " Game 2317 \n",
      "ResultType.NoTileRyuuKyoku: Totally 113 steps\n",
      " Game 2351 \n",
      "ResultType.TsumoAgari: Totally 92 steps\n",
      " Game 2380 \n",
      "ResultType.NoTileRyuuKyoku: Totally 124 steps\n",
      " Game 2468 \n",
      "ResultType.NoTileRyuuKyoku: Totally 113 steps\n",
      " Game 2503 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 2557 \n",
      "ResultType.NoTileRyuuKyoku: Totally 121 steps\n",
      " Game 2625 \n",
      "ResultType.NoTileRyuuKyoku: Totally 117 steps\n",
      " Game 2642 \n",
      "ResultType.NoTileRyuuKyoku: Totally 120 steps\n",
      " Game 2698 \n",
      "ResultType.NoTileRyuuKyoku: Totally 116 steps\n",
      " Game 2795 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 2879 \n",
      "ResultType.TsumoAgari: Totally 58 steps\n",
      " Game 2935 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 2942 \n",
      "ResultType.RonAgari: Totally 90 steps\n",
      " Game 2944 \n",
      "ResultType.NoTileRyuuKyoku: Totally 119 steps\n",
      " Game 2989 \n",
      "ResultType.RonAgari: Totally 80 steps\n",
      " Game 3202 \n",
      "ResultType.NoTileRyuuKyoku: Totally 113 steps\n",
      " Game 3216 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 3257 \n",
      "ResultType.NoTileRyuuKyoku: Totally 112 steps\n",
      " Game 3289 \n",
      "ResultType.TsumoAgari: Totally 84 steps\n",
      " Game 3386"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b7768aa2240a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start,\n\u001b[1;32m--> 205\u001b[1;33m                                     care_lose=False, logging=True)\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Codes\\mahjong\\AI\\aiFrost2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, symmetric_hand, episode_start, care_lose, logging)\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0mall_target_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_S\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_target_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Codes\\mahjong\\AI\\aiFrost2.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input, target, logging, global_step)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             loss, _ , summary = self.session.run([self.loss, self.train_step, self.merged],\n\u001b[1;32m--> 104\u001b[1;33m                 feed_dict = {self.matrix_features: input[0], self.vector_features: input[1], self.value_target: target})\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_games = 1000000\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    if n % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    \n",
    "    for i in range(4):\n",
    "        if agents[i].memory.filled_size >= episode_savebuffer and agents[i].memory.tail % episode_savebuffer == 0:\n",
    "            agents[i].memory.save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2\" + datetime_str + \".npz\")\n",
    "    \n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = [[], [], [], []]\n",
    "    episode_states = [[], [], [], []]\n",
    "    episode_rewards = [[], [], [], []]\n",
    "    episode_actions = [[], [], [], []]\n",
    "    episode_policies = [[], [], [], []]\n",
    "    \n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_actions[who].append(0)\n",
    "            policy = np.zeros([mu_size,], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            \n",
    "            if agents[who].memory.filled_size < episode_start:  # For collecting data only\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Riichi:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                        good_actions.append(a)\n",
    "            #######################################\n",
    "            \n",
    "            next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                           np.array(next_aval_states[1])[good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_actions[who].append(action)\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "#             step += 2\n",
    "            \n",
    "        elif what == \"response\":\n",
    "            policies = [np.zeros([mu_size,], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                next_aval_states = env.get_aval_next_states(i)\n",
    "                \n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                \n",
    "                if agents[i].memory.filled_size < episode_start:  # For collecting data only\n",
    "                    for a in range(len(aval_actions)):\n",
    "                        if aval_actions[a].action == mp.Action.Ron:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                            good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                                  np.array(next_aval_states[1])[good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                    \n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "                    \n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                \n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                next_states[i] = env.get_state_(i)\n",
    "                episode_dones[i].append(done)\n",
    "                episode_states[i].append(this_states[i])\n",
    "                episode_rewards[i].append(rs[i])\n",
    "                episode_actions[i].append(actions[i])\n",
    "                episode_policies[i].append(policies[i]) # only 1 available action (draw)\n",
    "        \n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            final_score_change = env.get_final_score_change()\n",
    "            \n",
    "            for i in range(4):\n",
    "                \n",
    "                agents[i].statistics(i, env.t.get_result(), env.get_final_score_change(), env.t.turn,\n",
    "                                     env.t.players[i].riichi, env.t.players[i].menchin)\n",
    "                \n",
    "                episode_states[i].append(env.get_state_(i))\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                               episode_dones[i], episode_actions[i],\n",
    "                                               episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "                try:\n",
    "                    with open(\"./Paipu/\" + datetime_str + \"game{}\".format(n) +\".txt\", 'w') as fp:\n",
    "                        fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if np.random.rand() < 0 + n / 250000: ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                                   episode_dones[i], episode_actions[i],\n",
    "                                                   episode_policies[i], weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                    \n",
    "            for n_train in range(5):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start,\n",
    "                                    care_lose=False, logging=True)\n",
    "            \n",
    "\n",
    "data = {\"rons\": env.final_score_changes, \"p0_stat\":agents[0].stat, \"p1_stat\":agents[1].stat,\n",
    "        \"p2_stat\":agents[2].stat, \"p3_stat\":agents[3].stat,}\n",
    "sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.final_score_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
