{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      memory=MahjongBufferFrost2(size=4096, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-1, 1),\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n",
    "\n",
    "\n",
    "episode_start = 256\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40\n",
    "max_steps = agents[0].memory.episode_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "for i in range(4):\n",
    "    buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost220190531-150059.pkl\"\n",
    "    agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../log/Agent0-20190531-150059-Game0/naiveAI.ckpt\n",
      "Model restored from../log/Agent0-20190531-150059-Game0/naiveAI.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../log/Agent1-20190531-150059-Game0/naiveAI.ckpt\n",
      "Model restored from../log/Agent1-20190531-150059-Game0/naiveAI.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../log/Agent2-20190531-150059-Game0/naiveAI.ckpt\n",
      "Model restored from../log/Agent2-20190531-150059-Game0/naiveAI.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../log/Agent3-20190531-150059-Game0/naiveAI.ckpt\n",
      "Model restored from../log/Agent3-20190531-150059-Game0/naiveAI.ckpt\n"
     ]
    }
   ],
   "source": [
    "# # example \n",
    "for i in range(4):\n",
    "    model_path =  \"../log/Agent{}\".format(i) + \"-20190531-150059-Game0/naiveAI.ckpt\"\n",
    "    agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Model saved in path: ../log/Agent0-20190601-165922-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent1-20190601-165922-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent2-20190601-165922-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent3-20190601-165922-Game0/naiveAI.ckpt\n",
      "Buffer saved in path: ./buffer/Agent0-MahjongBufferFrost220190601-165922.pkl\n",
      "Buffer saved in path: ./buffer/Agent1-MahjongBufferFrost220190601-165922.pkl\n",
      "Buffer saved in path: ./buffer/Agent2-MahjongBufferFrost220190601-165922.pkl\n",
      "Buffer saved in path: ./buffer/Agent3-MahjongBufferFrost220190601-165922.pkl\n",
      " Game 46"
     ]
    }
   ],
   "source": [
    "\n",
    "n_games = 1000000\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "\n",
    "    if n % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir=\"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(\n",
    "                n))  # save network parameters every 10000 episodes\n",
    "\n",
    "    for i in range(4):\n",
    "        if agents[i].memory.filled_size >= episode_savebuffer and agents[i].memory.tail % episode_savebuffer == 0:\n",
    "            agents[i].memory.save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2\" + datetime_str + \".pkl\")\n",
    "\n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = np.zeros([4, max_steps], dtype=np.float16)\n",
    "    episode_matrix_features = np.zeros([4, max_steps, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    episode_vector_features = np.zeros([4, max_steps, num_vf], dtype=np.float16)\n",
    "    episode_rewards = np.zeros([4, max_steps], dtype=np.float32)\n",
    "    episode_actions = np.zeros([4, max_steps], dtype=np.int32)\n",
    "    episode_policies = np.zeros([4, max_steps, mu_size], dtype=np.float32)\n",
    "\n",
    "\n",
    "    done = 0\n",
    "    #     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4, ], dtype=np.int32)\n",
    "    rs = np.zeros([4, ], dtype=np.float32)\n",
    "\n",
    "    this_states = env.reset()  ## for all players\n",
    "\n",
    "    next_aval_states = deepcopy(this_states)\n",
    "\n",
    "    next_matrix_features = np.zeros([4, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    next_vector_features = np.zeros([4, num_vf], dtype=np.float16)\n",
    "\n",
    "    next_states = [[], [], [], []]\n",
    "\n",
    "    step = 0\n",
    "    agent_step = [0, 0, 0, 0]\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "\n",
    "            ######################## Draw a tile #####################\n",
    "\n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "\n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = 0\n",
    "            policy = np.zeros([mu_size, ], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who, agent_step[who]] = policy  # only 1 available action (draw)\n",
    "\n",
    "            agent_step[who] += 1\n",
    "\n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "\n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "\n",
    "            if agents[who].memory.filled_size < episode_start:  # For collecting data only\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Riichi:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                        good_actions.append(a)\n",
    "            #######################################\n",
    "\n",
    "            next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                           np.array(next_aval_states[1])[good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "\n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "\n",
    "            next_states[who] = env.get_state_(who)\n",
    "\n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = action\n",
    "            episode_policies[who, agent_step[who]] = policy\n",
    "            agent_step[who] += 1\n",
    "\n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "\n",
    "        # step += 2\n",
    "\n",
    "        elif what == \"response\":\n",
    "            policies = [np.zeros([mu_size, ], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                next_aval_states = env.get_aval_next_states(i)\n",
    "\n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "\n",
    "                if agents[i].memory.filled_size < episode_start:  # For collecting data only\n",
    "                    for a in range(len(aval_actions)):\n",
    "                        if aval_actions[a].action == mp.Action.Ron:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                            good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                                  np.array(next_aval_states[1])[good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "\n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "\n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                episode_dones[i, agent_step[i]] = done\n",
    "                episode_matrix_features[i, agent_step[i]] = this_states[i][0]\n",
    "                episode_vector_features[i, agent_step[i]] = this_states[i][1]\n",
    "                episode_rewards[i, agent_step[i]] = rs[i]\n",
    "                episode_actions[i, agent_step[i]] = actions[i]\n",
    "                episode_policies[i, agent_step[i]] = policies[i]\n",
    "                agent_step[i] += 1\n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # print(\"Game {}, step {}\".format(n, step))\n",
    "        #         print(env.get_phase_text())\n",
    "\n",
    "        if done:\n",
    "\n",
    "\n",
    "\n",
    "            final_score_change = env.get_final_score_change()\n",
    "\n",
    "            for i in range(4):\n",
    "\n",
    "                agents[i].statistics(i, env.t.get_result(), env.get_final_score_change(), env.t.turn,\n",
    "                                     env.t.players[i].riichi, env.t.players[i].menchin)\n",
    "\n",
    "                current_state = env.get_state_(i)\n",
    "                episode_matrix_features[i, agent_step[i]] = current_state[0]\n",
    "                episode_vector_features[i, agent_step[i]] = current_state[1]\n",
    "\n",
    "                if len(episode_dones[i]) >= 1:  # if not 1st turn end\n",
    "                    episode_dones[i, -1] = 1\n",
    "\n",
    "\n",
    "            if not np.max(final_score_change) == 0:  ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                               episode_vector_features[i, 0: agent_step[i]],\n",
    "                                               episode_rewards[i, 0: agent_step[i]],\n",
    "                                               episode_dones[i, 0: agent_step[i]],\n",
    "                                               episode_actions[i, 0: agent_step[i]],\n",
    "                                               episode_policies[i, 0: agent_step[i]],\n",
    "                                               weight=0)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "\n",
    "                try:\n",
    "                    with open(\"./Paipu/\" + datetime_str + \"game{}\".format(n) + \".txt\", 'w') as fp:\n",
    "                        fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if np.random.rand() < 0.01 + n / 500000:  ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                                   episode_vector_features[i, 0: agent_step[i]],\n",
    "                                                   episode_rewards[i, 0: agent_step[i]],\n",
    "                                                   episode_dones[i, 0: agent_step[i]],\n",
    "                                                   episode_actions[i, 0: agent_step[i]],\n",
    "                                                   episode_policies[i, 0: agent_step[i]],\n",
    "                                                   weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "\n",
    "\n",
    "            for n_train in range(5):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start,\n",
    "                                    care_lose=False, logging=True, batch_size=32)\n",
    "\n",
    "\n",
    "data = {\"rons\": env.final_score_changes, \"p0_stat\": agents[0].stat, \"p1_stat\": agents[1].stat,\n",
    "        \"p2_stat\": agents[2].stat, \"p3_stat\": agents[3].stat, }\n",
    "sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.final_score_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
