{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dongqi Han\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      memory=MahjongBufferFrost2(size=1024, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-1, 1),\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example \n",
    "# model_path =  \"../log/Agent0-20190501-175203-Game0/naiveAI.ckpt\"\n",
    "# for i in range(4):\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "### This is for AI agents those only cares about itself, i.e., no defense. Therefore, there is no negative reward.\n",
    "\n",
    "### Also, 能和则和，能立直则立直"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Model saved in path: ../log/Agent0-20190520-202634-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent1-20190520-202634-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent2-20190520-202634-Game0/naiveAI.ckpt\n",
      "Model saved in path: ../log/Agent3-20190520-202634-Game0/naiveAI.ckpt\n",
      " Game 57 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 61 \n",
      "ResultType.NoTileRyuuKyoku: Totally 117 steps\n",
      " Game 80 \n",
      "ResultType.RonAgari: Totally 67 steps\n",
      " Game 86 \n",
      "ResultType.NoTileRyuuKyoku: Totally 117 steps\n",
      " Game 106 \n",
      "ResultType.NoTileRyuuKyoku: Totally 114 steps\n",
      " Game 160 \n",
      "ResultType.NoTileRyuuKyoku: Totally 117 steps\n",
      " Game 198 \n",
      "ResultType.NoTileRyuuKyoku: Totally 112 steps\n",
      " Game 254 \n",
      "ResultType.NoTileRyuuKyoku: Totally 118 steps\n",
      " Game 299 \n",
      "ResultType.NoTileRyuuKyoku: Totally 114 steps\n",
      " Game 361 \n",
      "ResultType.NoTileRyuuKyoku: Totally 121 steps\n",
      " Game 383 \n",
      "ResultType.NoTileRyuuKyoku: Totally 116 steps\n",
      " Game 412 \n",
      "ResultType.NoTileRyuuKyoku: Totally 122 steps\n",
      " Game 436 \n",
      "ResultType.NoTileRyuuKyoku: Totally 121 steps\n",
      " Game 441 \n",
      "ResultType.NoTileRyuuKyoku: Totally 115 steps\n",
      " Game 499"
     ]
    }
   ],
   "source": [
    "\n",
    "n_games = 1000000\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    if n % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = [[], [], [], []]\n",
    "    episode_states = [[], [], [], []]\n",
    "    episode_rewards = [[], [], [], []]\n",
    "    episode_policies = [[], [], [], []]\n",
    "    \n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_policies[who].append(np.array([1.])) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "#             for a in range(len(aval_actions)):\n",
    "#                 if aval_actions[a].action == mp.Action.Riichi:\n",
    "#                     good_actions.append(a)\n",
    "                    \n",
    "#                 if aval_actions[a].action == mp.Action.Tsumo:\n",
    "#                     good_actions.append(a)\n",
    "            #######################################\n",
    "            \n",
    "            next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([next_aval_states[0][good_actions], next_aval_states[1][good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "#             step += 2\n",
    "            \n",
    "        elif what == \"response\":\n",
    "            policies = [[], [], [], []]\n",
    "            for i in range(4):\n",
    "                next_aval_states = env.get_aval_next_states(i)\n",
    "                \n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "#                 for a in range(len(aval_actions)):\n",
    "#                     if aval_actions[a].action == mp.Action.Ron:\n",
    "#                         good_actions.append(a)\n",
    "                \n",
    "#                     if aval_actions[a].action == mp.Action.ChanKan:\n",
    "#                         good_actions.append(a)\n",
    "\n",
    "#                     if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "#                         good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([next_aval_states[0][good_actions], next_aval_states[1][good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "                \n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                next_states[i] = env.get_state_(i)\n",
    "                episode_dones[i].append(done)\n",
    "                episode_states[i].append(this_states[i])\n",
    "                episode_rewards[i].append(rs[i])\n",
    "                episode_policies[i].append(policies[i]) # only 1 available action (draw)\n",
    "        \n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                episode_states[i].append(env.get_state_(i))\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                               episode_dones[i], episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "#                 with open(\"./Paipu/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".txt\", 'w') as fp:\n",
    "#                     fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "#                     break\n",
    "            else:\n",
    "                if np.random.rand() < 0.025: ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                                   episode_dones[i], episode_policies[i], weight=1)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                    \n",
    "            for n_train in range(5):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=16, logging=True)\n",
    "            \n",
    "\n",
    "data = {\"rons\": env.final_score_changes}\n",
    "sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.t.get_selected_action_tile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yi qi guan tong\n",
    "hand_matrix_yiqi = \\\n",
    "[[1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] \n",
    "\n",
    "\n",
    "# pi hu\n",
    "hand_matrix_pihu = \\\n",
    "[[0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [1, 1, 1, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]]\n",
    "\n",
    "\n",
    "# lan pai\n",
    "hand_matrix_lan = \\\n",
    "[[1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] \n",
    "\n",
    "\n",
    "\n",
    "# yi qi guan tong 2\n",
    "hand_matrix_yiqi2 = \\\n",
    "[[0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一气贯通m\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_yiqi, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 屁胡\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_pihu, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不可能听牌的情况\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_lan, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一气贯通s\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_yiqi2, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
