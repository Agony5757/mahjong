{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "\n",
    "episode_start = 512\n",
    "episode_savebuffer = 128\n",
    "buffer_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf, value_base=10000),\n",
    "                      memory=MahjongBufferFrost2(size=buffer_size, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-2, 2), alpha=0.99,\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n",
    "mu_size = agents[0].memory.max_action_num\n",
    "max_steps = agents[0].memory.episode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost220190617-165104.pkl\"\n",
    "#     agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     model_path =  \"../log/Agent{}\".format(i) + \"-20190615-173403-Game299998/naiveAI.ckpt\"\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 1000000\n",
    "n = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 4 \n",
      "ResultType.TsumoAgari: Totally 149 steps\n",
      " Game 71 \n",
      "ResultType.NoTileRyuuKyoku: Totally 154 steps\n",
      " Game 77 \n",
      "ResultType.RonAgari: Totally 158 steps\n",
      " Game 144 \n",
      "ResultType.TsumoAgari: Totally 85 steps\n",
      " Game 183 \n",
      "ResultType.RonAgari: Totally 18 steps\n",
      " Game 302 \n",
      "ResultType.TsumoAgari: Totally 41 steps\n",
      " Game 316 \n",
      "ResultType.RonAgari: Totally 150 steps\n",
      " Game 337"
     ]
    }
   ],
   "source": [
    "print(\"Start!\")\n",
    "\n",
    "while n <= n_games:\n",
    "    n += 1\n",
    "#     try:\n",
    "    if (n + 1) % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir=\"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(\n",
    "                n - 1))  # save network parameters every 10000 episodes\n",
    "\n",
    "    for i in range(4):\n",
    "        if agents[i].memory.filled_size >= episode_savebuffer and agents[i].memory.tail % episode_savebuffer == 0:\n",
    "            agents[i].memory.save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2\" + datetime_str + \".pkl\")\n",
    "\n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = np.zeros([4, max_steps], dtype=np.float16)\n",
    "    episode_next_matrix_features = np.zeros([4, max_steps, mu_size, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    episode_next_vector_features = np.zeros([4, max_steps, mu_size, num_vf], dtype=np.float16)\n",
    "    episode_num_aval_actions = np.zeros([4, max_steps], dtype=np.int16)  # number of available actions\n",
    "    episode_rewards = np.zeros([4, max_steps], dtype=np.float32)\n",
    "    episode_actions = np.zeros([4, max_steps], dtype=np.int32)\n",
    "    episode_policies = np.zeros([4, max_steps, mu_size], dtype=np.float32)\n",
    "\n",
    "    done = 0\n",
    "    #     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4, ], dtype=np.int32)\n",
    "    rs = np.zeros([4, ], dtype=np.float32)\n",
    "    aval_actions_lens = np.zeros([4, ], dtype=int)\n",
    "\n",
    "    this_states = env.reset()  ## for all players\n",
    "\n",
    "    #     next_aval_states = deepcopy(this_states)\n",
    "\n",
    "    aval_next_matrix_features = np.zeros([4, mu_size, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    aval_next_vector_features = np.zeros([4, mu_size, num_vf], dtype=np.float16)\n",
    "\n",
    "    next_states = [[], [], [], []]\n",
    "\n",
    "    step = 0\n",
    "    agent_step = [0, 0, 0, 0]\n",
    "\n",
    "    while not done and step < 1000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "\n",
    "            #             if agents[who].memory.filled_size < episode_start:  # For collecting data only\n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "\n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            #######################################\n",
    "\n",
    "            next_aval_matrix_states, next_aval_vector_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            next_aval_states = (next_aval_matrix_states, next_aval_vector_states)\n",
    "\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([np.array(next_aval_matrix_states)[good_actions],\n",
    "                                                           np.array(next_aval_vector_states)[good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "\n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "\n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_next_matrix_features[who, agent_step[who], 0:len(aval_actions)] = next_aval_matrix_states\n",
    "            episode_next_vector_features[who, agent_step[who], 0:len(aval_actions)] = next_aval_vector_states\n",
    "            episode_num_aval_actions[who, agent_step[who]] = len(aval_actions)\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = action\n",
    "            episode_policies[who, agent_step[who]] = policy\n",
    "            agent_step[who] += 1\n",
    "\n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "\n",
    "\n",
    "        elif what == \"response\":\n",
    "            policies = [np.zeros([mu_size, ], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                next_aval_matrix_states, next_aval_vector_states = env.get_aval_next_states(i)  ## for a single player\n",
    "                next_aval_states = (next_aval_matrix_states, next_aval_vector_states)\n",
    "\n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "\n",
    "                aval_actions_lens[i] = len(aval_actions)\n",
    "                episode_next_matrix_features[i, agent_step[i], 0:aval_actions_lens[i]] = next_aval_matrix_states\n",
    "                episode_next_vector_features[i, agent_step[i], 0:aval_actions_lens[i]] = next_aval_vector_states\n",
    "\n",
    "                good_actions = []\n",
    "\n",
    "                #                 if agents[i].memory.filled_size < episode_start:  # For collecting data only\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([np.array(next_aval_matrix_states)[good_actions],\n",
    "                                                                  np.array(next_aval_vector_states)[good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "\n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "\n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                episode_num_aval_actions[i, agent_step[i]] = aval_actions_lens[i]\n",
    "                episode_dones[i, agent_step[i]] = done\n",
    "                episode_rewards[i, agent_step[i]] = rs[i]\n",
    "                episode_actions[i, agent_step[i]] = actions[i]\n",
    "                episode_policies[i, agent_step[i]] = policies[i]\n",
    "                agent_step[i] += 1\n",
    "\n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "\n",
    "        step += 1\n",
    "        #         if done or step == max_steps:\n",
    "        #             print('done = {}'.format(done))\n",
    "        #             print(env.get_phase_text())\n",
    "\n",
    "\n",
    "        #         print(\"Game {}, step {}\".format(n, step))\n",
    "\n",
    "\n",
    "        if env.t.get_phase() == 16:  # GAME_OVER\n",
    "\n",
    "            final_score_change = env.get_final_score_change()\n",
    "\n",
    "            for i in range(4):\n",
    "\n",
    "                agents[i].statistics(i, env.t.get_result(), env.get_final_score_change(), env.t.turn,\n",
    "                                     env.t.players[i].riichi, env.t.players[i].menchin)\n",
    "\n",
    "                if agent_step[i] >= 1:  # if not 1st turn end\n",
    "                    episode_dones[i, agent_step[i] - 1] = 1\n",
    "                    episode_rewards[i, agent_step[i] - 1] = final_score_change[i]\n",
    "\n",
    "            if not np.max(final_score_change) == 0:  ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_num_aval_actions[i],\n",
    "                                               episode_next_matrix_features[i],\n",
    "                                               episode_next_vector_features[i],\n",
    "                                               episode_rewards[i],\n",
    "                                               episode_dones[i],\n",
    "                                               episode_actions[i],\n",
    "                                               episode_policies[i],\n",
    "                                               weight=0)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(step))\n",
    "\n",
    "                try:\n",
    "                    with open(\"./Paipu/\" + \"{}p\".format(int(np.max(final_score_change))) + datetime_str + \"game{}\".format(n) + \".txt\", 'w') as fp:\n",
    "                        fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if np.random.rand() < 0.005 + (n / 500000) ** 2:  ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_num_aval_actions[i],\n",
    "                                                   episode_next_matrix_features[i],\n",
    "                                                   episode_next_vector_features[i],\n",
    "                                                   episode_rewards[i],\n",
    "                                                   episode_dones[i],\n",
    "                                                   episode_actions[i],\n",
    "                                                   episode_policies[i],\n",
    "                                                   weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(step))\n",
    "\n",
    "            for n_train in range(4):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start,\n",
    "                                    care_lose=False, logging=True)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "\n",
    "data = {\"rons\": env.final_score_changes, \"p0_stat\": agents[0].stat, \"p1_stat\": agents[1].stat,\n",
    "        \"p2_stat\": agents[2].stat, \"p3_stat\": agents[3].stat, }\n",
    "sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_S, test_s = agents[0].memory.Sp[11].todense()[12,0:20,:].reshape([-1, 34, 58]), agents[0].memory.sp[11][12,0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as scisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred = agents[0].nn.output([test_S, test_s]).reshape([-1])\n",
    "print(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred = agents[i].nn.output(next_aval_states).reshape([-1])\n",
    "print(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 100\n",
    "scisp.softmax([b * 0.22, b * 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents[3].greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([[1,2,3, -np.inf], [1, -np.inf, -np.inf, -np.inf]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scisp.softmax(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agents[1].memory.d[2:5, :].transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.r[12, :agents[1].memory.length[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.d[12, :agents[1].memory.length[12]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.n[12, :agents[1].memory.length[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_num_aval_actions[0][90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_selected_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_next_aval_states_matrix_features_frost2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_aval_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_self_actions()[-1].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    agents[i].remember_episode(episode_num_aval_actions[i, 0: agent_step[i]],\n",
    "                               episode_next_matrix_features[i, 0: agent_step[i]],\n",
    "                               episode_next_vector_features[i, 0: agent_step[i]],\n",
    "                               episode_rewards[i, 0: agent_step[i]],\n",
    "                               episode_dones[i, 0: agent_step[i]],\n",
    "                               episode_actions[i, 0: agent_step[i]],\n",
    "                               episode_policies[i, 0: agent_step[i]],\n",
    "                               weight=0)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.final_score_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
