{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "\n",
    "episode_start = 512\n",
    "episode_savebuffer = 128\n",
    "buffer_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf, value_base=10000),\n",
    "                      memory=MahjongBufferFrost2(size=buffer_size, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-2, 2), alpha=0.99,\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n",
    "mu_size = agents[0].memory.max_action_num\n",
    "max_steps = agents[0].memory.episode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost220190617-165104.pkl\"\n",
    "#     agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     model_path =  \"../log/Agent{}\".format(i) + \"-20190615-173403-Game299998/naiveAI.ckpt\"\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 1000000\n",
    "n = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 36287 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36288 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36306 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 36329 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36330 \n",
      "ResultType.TsumoAgari: Totally 99 steps\n",
      " Game 36340 \n",
      "ResultType.RonAgari: Totally 118 steps\n",
      " Game 36341 \n",
      "ResultType.NoTileRyuuKyoku: Totally 144 steps\n",
      " Game 36346 \n",
      "ResultType.TsumoAgari: Totally 99 steps\n",
      " Game 36352 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36362 \n",
      "ResultType.RonAgari: Totally 50 steps\n",
      " Game 36375 \n",
      "ResultType.RonAgari: Totally 138 steps\n",
      " Game 36376 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36378 \n",
      "ResultType.RonAgari: Totally 92 steps\n",
      " Game 36381 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36393 \n",
      "ResultType.TsumoAgari: Totally 51 steps\n",
      " Game 36394 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 36396 \n",
      "ResultType.RonAgari: Totally 66 steps\n",
      " Game 36398 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36402 \n",
      "ResultType.TsumoAgari: Totally 121 steps\n",
      " Game 36412 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 36418 \n",
      "ResultType.NoTileRyuuKyoku: Totally 144 steps\n",
      " Game 36419 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36420 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36443 \n",
      "ResultType.TsumoAgari: Totally 139 steps\n",
      " Game 36445 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36455 \n",
      "ResultType.TsumoAgari: Totally 91 steps\n",
      " Game 36460 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36476 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36479 \n",
      "ResultType.RonAgari: Totally 20 steps\n",
      " Game 36484 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36486 \n",
      "ResultType.TsumoAgari: Totally 135 steps\n",
      " Game 36497 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 36501 \n",
      "ResultType.RonAgari: Totally 46 steps\n",
      " Game 36502 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36512 \n",
      "ResultType.RonAgari: Totally 120 steps\n",
      " Game 36514 \n",
      "ResultType.RonAgari: Totally 140 steps\n",
      " Game 36521 \n",
      "ResultType.RonAgari: Totally 138 steps\n",
      " Game 36527 \n",
      "ResultType.TsumoAgari: Totally 109 steps\n",
      " Game 36529 \n",
      "ResultType.RonAgari: Totally 94 steps\n",
      " Game 36533 \n",
      "ResultType.TsumoAgari: Totally 65 steps\n",
      " Game 36553 \n",
      "ResultType.RonAgari: Totally 126 steps\n",
      " Game 36556 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36563 \n",
      "ResultType.TsumoAgari: Totally 101 steps\n",
      " Game 36573 \n",
      "ResultType.RonAgari: Totally 110 steps\n",
      " Game 36578 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36580 \n",
      "ResultType.RonAgari: Totally 122 steps\n",
      " Game 36581 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36585 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36600 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 36603 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 36610 \n",
      "ResultType.TsumoAgari: Totally 121 steps\n",
      " Game 36612 \n",
      "ResultType.RonAgari: Totally 136 steps\n",
      " Game 36613 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36618 \n",
      "ResultType.TsumoAgari: Totally 49 steps\n",
      " Game 36625 \n",
      "ResultType.TsumoAgari: Totally 81 steps\n",
      " Game 36637 \n",
      "ResultType.RonAgari: Totally 72 steps\n",
      " Game 36644 \n",
      "ResultType.NoTileRyuuKyoku: Totally 144 steps\n",
      " Game 36648 \n",
      "ResultType.RonAgari: Totally 98 steps\n",
      " Game 36650 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36653 \n",
      "ResultType.TsumoAgari: Totally 139 steps\n",
      " Game 36666 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36675 \n",
      "ResultType.TsumoAgari: Totally 117 steps\n",
      " Game 36676 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36686 \n",
      "ResultType.RonAgari: Totally 122 steps\n",
      " Game 36692 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36694 \n",
      "ResultType.RonAgari: Totally 68 steps\n",
      " Game 36707 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36709 \n",
      "ResultType.NoTileRyuuKyoku: Totally 144 steps\n",
      " Game 36727 \n",
      "ResultType.RonAgari: Totally 54 steps\n",
      " Game 36732 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36765 \n",
      "ResultType.TsumoAgari: Totally 125 steps\n",
      " Game 36767 \n",
      "ResultType.TsumoAgari: Totally 89 steps\n",
      " Game 36771 \n",
      "ResultType.RonAgari: Totally 46 steps\n",
      "Buffer saved in path: ./buffer/Agent0-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 36775 \n",
      "ResultType.TsumoAgari: Totally 107 steps\n",
      "Buffer saved in path: ./buffer/Agent1-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 36779 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      "Buffer saved in path: ./buffer/Agent2-MahjongBufferFrost220190618-205517.pkl\n",
      "Buffer saved in path: ./buffer/Agent3-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 36787 \n",
      "ResultType.RonAgari: Totally 136 steps\n",
      " Game 36792 \n",
      "ResultType.TsumoAgari: Totally 63 steps\n",
      " Game 36797 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36801 \n",
      "ResultType.TsumoAgari: Totally 39 steps\n",
      " Game 36805 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36807 \n",
      "ResultType.TsumoAgari: Totally 133 steps\n",
      " Game 36810 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36811 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36817 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36826 \n",
      "ResultType.RonAgari: Totally 54 steps\n",
      " Game 36834 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36848 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36851 \n",
      "ResultType.RonAgari: Totally 86 steps\n",
      " Game 36852 \n",
      "ResultType.RonAgari: Totally 136 steps\n",
      " Game 36855 \n",
      "ResultType.TsumoAgari: Totally 139 steps\n",
      " Game 36861 \n",
      "ResultType.RonAgari: Totally 90 steps\n",
      " Game 36863 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36872 \n",
      "ResultType.RonAgari: Totally 74 steps\n",
      " Game 36879 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36885 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36888 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36889 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 36893 \n",
      "ResultType.RonAgari: Totally 54 steps\n",
      " Game 36898 \n",
      "ResultType.RonAgari: Totally 78 steps\n",
      " Game 36900 \n",
      "ResultType.TsumoAgari: Totally 125 steps\n",
      " Game 36904 \n",
      "ResultType.RonAgari: Totally 124 steps\n",
      " Game 36908 \n",
      "ResultType.TsumoAgari: Totally 127 steps\n",
      " Game 36913 \n",
      "ResultType.RonAgari: Totally 138 steps\n",
      " Game 36916 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 36918 \n",
      "ResultType.RonAgari: Totally 110 steps\n",
      " Game 36945 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36965 \n",
      "ResultType.RonAgari: Totally 140 steps\n",
      " Game 36967 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36974 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36988 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36990 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 36996 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37003 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37009 \n",
      "ResultType.TsumoAgari: Totally 53 steps\n",
      " Game 37017 \n",
      "ResultType.RonAgari: Totally 134 steps\n",
      " Game 37034 \n",
      "ResultType.RonAgari: Totally 98 steps\n",
      " Game 37038 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37073 \n",
      "ResultType.RonAgari: Totally 56 steps\n",
      " Game 37075 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37078 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37079 \n",
      "ResultType.RonAgari: Totally 120 steps\n",
      " Game 37083 \n",
      "ResultType.TsumoAgari: Totally 115 steps\n",
      " Game 37088 \n",
      "ResultType.TsumoAgari: Totally 145 steps\n",
      " Game 37090 \n",
      "ResultType.TsumoAgari: Totally 137 steps\n",
      " Game 37092 \n",
      "ResultType.TsumoAgari: Totally 121 steps\n",
      " Game 37109 \n",
      "ResultType.RonAgari: Totally 52 steps\n",
      " Game 37112 \n",
      "ResultType.TsumoAgari: Totally 123 steps\n",
      " Game 37118 \n",
      "ResultType.RonAgari: Totally 132 steps\n",
      " Game 37119 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37122 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 37148 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37159 \n",
      "ResultType.TsumoAgari: Totally 129 steps\n",
      " Game 37161 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37166 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 37170 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37171 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37185 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37197 \n",
      "ResultType.RonAgari: Totally 102 steps\n",
      " Game 37202 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 37212 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37213 \n",
      "ResultType.RonAgari: Totally 144 steps\n",
      " Game 37228 \n",
      "ResultType.RonAgari: Totally 100 steps\n",
      " Game 37241 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37245 \n",
      "ResultType.TsumoAgari: Totally 129 steps\n",
      " Game 37253 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37263 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37274 \n",
      "ResultType.TsumoAgari: Totally 109 steps\n",
      " Game 37275 \n",
      "ResultType.TsumoAgari: Totally 117 steps\n",
      " Game 37282 \n",
      "ResultType.TsumoAgari: Totally 109 steps\n",
      " Game 37304 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37315 \n",
      "ResultType.TsumoAgari: Totally 83 steps\n",
      " Game 37324 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 37330 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37341 \n",
      "ResultType.TsumoAgari: Totally 89 steps\n",
      " Game 37342 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 37343 \n",
      "ResultType.TsumoAgari: Totally 105 steps\n",
      " Game 37346 \n",
      "ResultType.RonAgari: Totally 50 steps\n",
      " Game 37350 \n",
      "ResultType.RonAgari: Totally 64 steps\n",
      " Game 37354 \n",
      "ResultType.RonAgari: Totally 60 steps\n",
      " Game 37355 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37362 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37368 \n",
      "ResultType.TsumoAgari: Totally 145 steps\n",
      " Game 37373 \n",
      "ResultType.TsumoAgari: Totally 93 steps\n",
      " Game 37378 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37390 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37394 \n",
      "ResultType.TsumoAgari: Totally 95 steps\n",
      " Game 37395 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37411 \n",
      "ResultType.RonAgari: Totally 108 steps\n",
      " Game 37414 \n",
      "ResultType.RonAgari: Totally 138 steps\n",
      " Game 37415 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37425 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37426 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 37428 \n",
      "ResultType.RonAgari: Totally 84 steps\n",
      " Game 37430 \n",
      "ResultType.RonAgari: Totally 98 steps\n",
      " Game 37435 \n",
      "ResultType.RonAgari: Totally 102 steps\n",
      " Game 37437 \n",
      "ResultType.RonAgari: Totally 110 steps\n",
      " Game 37439 \n",
      "ResultType.RonAgari: Totally 114 steps\n",
      " Game 37446 \n",
      "ResultType.RonAgari: Totally 82 steps\n",
      " Game 37447 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 37451 \n",
      "ResultType.TsumoAgari: Totally 97 steps\n",
      " Game 37467 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37468 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37489 \n",
      "ResultType.RonAgari: Totally 110 steps\n",
      " Game 37491 \n",
      "ResultType.RonAgari: Totally 98 steps\n",
      " Game 37495 \n",
      "ResultType.TsumoAgari: Totally 101 steps\n",
      " Game 37499 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37516 \n",
      "ResultType.RonAgari: Totally 130 steps\n",
      " Game 37521 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37528 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37533 \n",
      "ResultType.TsumoAgari: Totally 105 steps\n",
      " Game 37538 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37544 \n",
      "ResultType.TsumoAgari: Totally 113 steps\n",
      " Game 37547 \n",
      "ResultType.RonAgari: Totally 96 steps\n",
      " Game 37552 \n",
      "ResultType.RonAgari: Totally 138 steps\n",
      " Game 37556 \n",
      "ResultType.RonAgari: Totally 104 steps\n",
      " Game 37559 \n",
      "ResultType.RonAgari: Totally 124 steps\n",
      " Game 37565 \n",
      "ResultType.TsumoAgari: Totally 97 steps\n",
      " Game 37570 \n",
      "ResultType.TsumoAgari: Totally 75 steps\n",
      " Game 37583 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37586 \n",
      "ResultType.TsumoAgari: Totally 135 steps\n",
      " Game 37601 \n",
      "ResultType.TsumoAgari: Totally 119 steps\n",
      "Buffer saved in path: ./buffer/Agent0-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 37604 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      "Buffer saved in path: ./buffer/Agent1-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 37607 \n",
      "ResultType.TsumoAgari: Totally 117 steps\n",
      "Buffer saved in path: ./buffer/Agent2-MahjongBufferFrost220190618-205517.pkl\n",
      "Buffer saved in path: ./buffer/Agent3-MahjongBufferFrost220190618-205517.pkl\n",
      " Game 37609 \n",
      "ResultType.RonAgari: Totally 122 steps\n",
      " Game 37612 \n",
      "ResultType.NoTileRyuuKyoku: Totally 146 steps\n",
      " Game 37621 \n",
      "ResultType.TsumoAgari: Totally 119 steps\n",
      " Game 37636 \n",
      "ResultType.TsumoAgari: Totally 89 steps\n",
      " Game 37647 \n",
      "ResultType.TsumoAgari: Totally 65 steps\n",
      " Game 37656 \n",
      "ResultType.TsumoAgari: Totally 61 steps\n",
      " Game 37674 \n",
      "ResultType.RonAgari: Totally 88 steps\n",
      " Game 37676 \n",
      "ResultType.NoTileRyuuKyoku: Totally 142 steps\n",
      " Game 37689 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37693 \n",
      "ResultType.TsumoAgari: Totally 113 steps\n",
      " Game 37704 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37705 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37710 \n",
      "ResultType.TsumoAgari: Totally 129 steps\n",
      " Game 37713 \n",
      "ResultType.TsumoAgari: Totally 139 steps\n",
      " Game 37716 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37719 \n",
      "ResultType.TsumoAgari: Totally 9 steps\n",
      " Game 37721 \n",
      "ResultType.TsumoAgari: Totally 135 steps\n",
      " Game 37722 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37735 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37736 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37737 \n",
      "ResultType.TsumoAgari: Totally 99 steps\n",
      " Game 37739 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37742 \n",
      "ResultType.RonAgari: Totally 42 steps\n",
      " Game 37762 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37764 \n",
      "ResultType.TsumoAgari: Totally 117 steps\n",
      " Game 37766 \n",
      "ResultType.RonAgari: Totally 32 steps\n",
      " Game 37775 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37788 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37789 \n",
      "ResultType.TsumoAgari: Totally 131 steps\n",
      " Game 37794 \n",
      "ResultType.TsumoAgari: Totally 85 steps\n",
      " Game 37796 \n",
      "ResultType.NoTileRyuuKyoku: Totally 140 steps\n",
      " Game 37799 \n",
      "ResultType.RonAgari: Totally 68 steps\n"
     ]
    }
   ],
   "source": [
    "print(\"Start!\")\n",
    "\n",
    "while n <= n_games:\n",
    "    n += 1\n",
    "#     try:\n",
    "    if (n + 1) % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir=\"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(\n",
    "                n - 1))  # save network parameters every 10000 episodes\n",
    "\n",
    "    for i in range(4):\n",
    "        if agents[i].memory.filled_size >= episode_savebuffer and agents[i].memory.tail % episode_savebuffer == 0:\n",
    "            agents[i].memory.save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2\" + datetime_str + \".pkl\")\n",
    "\n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = np.zeros([4, max_steps], dtype=np.float16)\n",
    "    episode_next_matrix_features = np.zeros([4, max_steps, mu_size, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    episode_next_vector_features = np.zeros([4, max_steps, mu_size, num_vf], dtype=np.float16)\n",
    "    episode_num_aval_actions = np.zeros([4, max_steps], dtype=np.int16)  # number of available actions\n",
    "    episode_rewards = np.zeros([4, max_steps], dtype=np.float32)\n",
    "    episode_actions = np.zeros([4, max_steps], dtype=np.int32)\n",
    "    episode_policies = np.zeros([4, max_steps, mu_size], dtype=np.float32)\n",
    "\n",
    "    done = 0\n",
    "    #     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4, ], dtype=np.int32)\n",
    "    rs = np.zeros([4, ], dtype=np.float32)\n",
    "    aval_actions_lens = np.zeros([4, ], dtype=int)\n",
    "\n",
    "    this_states = env.reset()  ## for all players\n",
    "\n",
    "    #     next_aval_states = deepcopy(this_states)\n",
    "\n",
    "    aval_next_matrix_features = np.zeros([4, mu_size, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    aval_next_vector_features = np.zeros([4, mu_size, num_vf], dtype=np.float16)\n",
    "\n",
    "    next_states = [[], [], [], []]\n",
    "\n",
    "    step = 0\n",
    "    agent_step = [0, 0, 0, 0]\n",
    "\n",
    "    while not done and step < 1000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "\n",
    "            #             if agents[who].memory.filled_size < episode_start:  # For collecting data only\n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "\n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            #######################################\n",
    "\n",
    "            next_aval_matrix_states, next_aval_vector_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            next_aval_states = (next_aval_matrix_states, next_aval_vector_states)\n",
    "\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([np.array(next_aval_matrix_states)[good_actions],\n",
    "                                                           np.array(next_aval_vector_states)[good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "\n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "\n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_next_matrix_features[who, agent_step[who], 0:len(aval_actions)] = next_aval_matrix_states\n",
    "            episode_next_vector_features[who, agent_step[who], 0:len(aval_actions)] = next_aval_vector_states\n",
    "            episode_num_aval_actions[who, agent_step[who]] = len(aval_actions)\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = action\n",
    "            episode_policies[who, agent_step[who]] = policy\n",
    "            agent_step[who] += 1\n",
    "\n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "\n",
    "\n",
    "        elif what == \"response\":\n",
    "            policies = [np.zeros([mu_size, ], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                next_aval_matrix_states, next_aval_vector_states = env.get_aval_next_states(i)  ## for a single player\n",
    "                next_aval_states = (next_aval_matrix_states, next_aval_vector_states)\n",
    "\n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "\n",
    "                aval_actions_lens[i] = len(aval_actions)\n",
    "                episode_next_matrix_features[i, agent_step[i], 0:aval_actions_lens[i]] = next_aval_matrix_states\n",
    "                episode_next_vector_features[i, agent_step[i], 0:aval_actions_lens[i]] = next_aval_vector_states\n",
    "\n",
    "                good_actions = []\n",
    "\n",
    "                #                 if agents[i].memory.filled_size < episode_start:  # For collecting data only\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([np.array(next_aval_matrix_states)[good_actions],\n",
    "                                                                  np.array(next_aval_vector_states)[good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size, ], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "\n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "\n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                episode_num_aval_actions[i, agent_step[i]] = aval_actions_lens[i]\n",
    "                episode_dones[i, agent_step[i]] = done\n",
    "                episode_rewards[i, agent_step[i]] = rs[i]\n",
    "                episode_actions[i, agent_step[i]] = actions[i]\n",
    "                episode_policies[i, agent_step[i]] = policies[i]\n",
    "                agent_step[i] += 1\n",
    "\n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "\n",
    "        step += 1\n",
    "        #         if done or step == max_steps:\n",
    "        #             print('done = {}'.format(done))\n",
    "        #             print(env.get_phase_text())\n",
    "\n",
    "\n",
    "        #         print(\"Game {}, step {}\".format(n, step))\n",
    "\n",
    "\n",
    "        if env.t.get_phase() == 16:  # GAME_OVER\n",
    "\n",
    "            final_score_change = env.get_final_score_change()\n",
    "\n",
    "            for i in range(4):\n",
    "\n",
    "                agents[i].statistics(i, env.t.get_result(), env.get_final_score_change(), env.t.turn,\n",
    "                                     env.t.players[i].riichi, env.t.players[i].menchin)\n",
    "\n",
    "                if agent_step[i] >= 1:  # if not 1st turn end\n",
    "                    episode_dones[i, agent_step[i] - 1] = 1\n",
    "                    episode_rewards[i, agent_step[i] - 1] = final_score_change[i]\n",
    "\n",
    "            if not np.max(final_score_change) == 0:  ## score change\n",
    "                for i in range(4):\n",
    "                    if agent_step[i] >= 1: # avoid 九种九牌\n",
    "                        agents[i].remember_episode(episode_num_aval_actions[i],\n",
    "                                                   episode_next_matrix_features[i],\n",
    "                                                   episode_next_vector_features[i],\n",
    "                                                   episode_rewards[i],\n",
    "                                                   episode_dones[i],\n",
    "                                                   episode_actions[i],\n",
    "                                                   episode_policies[i],\n",
    "                                                   weight=0)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(step))\n",
    "\n",
    "                try:\n",
    "                    with open(\"./Paipu/\" + \"{}p\".format(int(np.max(final_score_change))) + datetime_str + \"game{}\".format(n) + \".txt\", 'w') as fp:\n",
    "                        fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                if np.random.rand() < 0.005 + (n / 500000):  ## no score change\n",
    "                    for i in range(4):\n",
    "                        if agent_step[i] >= 1:  # avoid 九种九牌\n",
    "                            agents[i].remember_episode(episode_num_aval_actions[i],\n",
    "                                                       episode_next_matrix_features[i],\n",
    "                                                       episode_next_vector_features[i],\n",
    "                                                       episode_rewards[i],\n",
    "                                                       episode_dones[i],\n",
    "                                                       episode_actions[i],\n",
    "                                                       episode_policies[i],\n",
    "                                                       weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(step))\n",
    "\n",
    "            for n_train in range(4):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start,\n",
    "                                    care_lose=False, logging=True)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "\n",
    "data = {\"rons\": env.final_score_changes, \"p0_stat\": agents[0].stat, \"p1_stat\": agents[1].stat,\n",
    "        \"p2_stat\": agents[2].stat, \"p3_stat\": agents[3].stat, }\n",
    "sio.savemat(\"./stats\" + datetime_str + \".mat\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_S, test_s = agents[0].memory.Sp[11].todense()[12,0:20,:].reshape([-1, 34, 58]), agents[0].memory.sp[11][12,0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special as scisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred = agents[0].nn.output([test_S, test_s]).reshape([-1])\n",
    "print(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred = agents[i].nn.output(next_aval_states).reshape([-1])\n",
    "print(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 100\n",
    "scisp.softmax([b * 0.22, b * 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents[3].greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value_pred / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([[1,2,3, -np.inf], [1, -np.inf, -np.inf, -np.inf]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scisp.softmax(next_value_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agents[1].memory.d[2:5, :].transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.r[12, :agents[1].memory.length[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.d[12, :agents[1].memory.length[12]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agents[1].memory.n[12, :agents[1].memory.length[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_num_aval_actions[0][90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_selected_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_next_aval_states_matrix_features_frost2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_aval_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.get_self_actions()[-1].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    agents[i].remember_episode(episode_num_aval_actions[i, 0: agent_step[i]],\n",
    "                               episode_next_matrix_features[i, 0: agent_step[i]],\n",
    "                               episode_next_vector_features[i, 0: agent_step[i]],\n",
    "                               episode_rewards[i, 0: agent_step[i]],\n",
    "                               episode_dones[i, 0: agent_step[i]],\n",
    "                               episode_actions[i, 0: agent_step[i]],\n",
    "                               episode_policies[i, 0: agent_step[i]],\n",
    "                               weight=0)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.final_score_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
