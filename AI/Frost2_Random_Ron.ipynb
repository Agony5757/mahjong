{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "memories = [MahjongBufferFrost2(size=4096, num_tile_type=num_tile_type, num_each_tile=num_each_tile,\n",
    "                                num_vf=num_vf) for i in range(4)]\n",
    "\n",
    "episode_start = 256\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40\n",
    "max_steps = memories[0].episode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 44 \n",
      "ResultType.TsumoAgari: Totally 150 steps\n",
      " Game 118"
     ]
    }
   ],
   "source": [
    "n_games = 100000\n",
    "\n",
    "process_time = 0\n",
    "learn_time = 0 \n",
    "select_time = 0\n",
    "all_time = 0\n",
    "play_time = 0\n",
    "response_time = 0\n",
    "copy_time = 0\n",
    "done_time = 0\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    st_all =  time.time()\n",
    "    \n",
    "#     if n % 10000 == 0:\n",
    "#         for i in range(4):\n",
    "#             agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    \n",
    "    for i in range(4):\n",
    "        if memories[i].tail % episode_savebuffer == 0:\n",
    "            memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_\" + datetime_str + \".pkl\")\n",
    "    \n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = np.zeros([4, max_steps], dtype=np.float16)\n",
    "    episode_matrix_features = np.zeros([4, max_steps, num_tile_type, num_each_tile], dtype=np.float16)\n",
    "    episode_vector_features = np.zeros([4, max_steps, num_vf], dtype=np.float16)\n",
    "    episode_rewards = np.zeros([4, max_steps], dtype=np.float32)\n",
    "    episode_actions = np.zeros([4, max_steps], dtype=np.int32)\n",
    "    episode_policies = np.zeros([4, max_steps, mu_size], dtype=np.float32)\n",
    "\n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    agent_step = [0, 0, 0, 0]\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        st_play = time.time()\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = 0\n",
    "            policy = np.zeros([mu_size, ], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who, agent_step[who]] = policy  # only 1 available action (draw)\n",
    "\n",
    "            agent_step[who] += 1\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            \n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "\n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            #######################################\n",
    "#             st_process = time.time()\n",
    "\n",
    "#             next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "#             et_process = time.time()\n",
    "#             process_time += et_process - st_process     \n",
    "            \n",
    "            st = time.time()\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                \n",
    "                a_in_good_as = np.random.choice(len(good_actions))\n",
    "                policy = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                \n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action = np.random.choice(len(aval_actions))\n",
    "                policy = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            \n",
    "            et = time.time()\n",
    "            select_time += et - st\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who, agent_step[who]] = done\n",
    "            episode_matrix_features[who, agent_step[who]] = this_states[who][0]\n",
    "            episode_vector_features[who, agent_step[who]] = this_states[who][1]\n",
    "            episode_rewards[who, agent_step[who]] = r\n",
    "            episode_actions[who, agent_step[who]] = action\n",
    "            episode_policies[who, agent_step[who]] = policy\n",
    "            agent_step[who] += 1\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            et_play = time.time()\n",
    "            play_time += et_play - st_play\n",
    "#             step += 2\n",
    "        \n",
    "        st_response = time.time()\n",
    "        if what == \"response\":\n",
    "            policies = [np.zeros([mu_size,], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                \n",
    "#                 st_process = time.time()\n",
    "#                 next_aval_states = env.get_aval_next_states(i)\n",
    "#                 et_process = time.time()\n",
    "#                 process_time += et_process - st_process\n",
    "                \n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                \n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                st = time.time()\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as = np.random.choice(len(good_actions))\n",
    "                    policies[i] = np.ones(len(good_actions), dtype=np.float32) / len(good_actions)\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                    \n",
    "                else:\n",
    "                    actions[i] = np.random.choice(len(aval_actions))\n",
    "                    policies[i] = np.ones(len(aval_actions), dtype=np.float32) / len(aval_actions)\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                \n",
    "                et = time.time()\n",
    "                select_time += et - st\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                episode_dones[i, agent_step[i]] = done\n",
    "                episode_matrix_features[i, agent_step[i]] = this_states[i][0]\n",
    "                episode_vector_features[i, agent_step[i]] = this_states[i][1]\n",
    "                episode_rewards[i, agent_step[i]] = rs[i]\n",
    "                episode_actions[i, agent_step[i]] = actions[i]\n",
    "                episode_policies[i, agent_step[i]] = policies[i]\n",
    "                agent_step[i] += 1\n",
    "                \n",
    "            ## next step\n",
    "            st_copy = time.time()\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            et_copy = time.time()\n",
    "            copy_time += et_copy - st_copy\n",
    "            \n",
    "            step += 1\n",
    "        et_response = time.time()\n",
    "        response_time += et_response - st_response\n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            st_done = time.time()\n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                current_state = env.get_state_(i)\n",
    "                episode_matrix_features[i, agent_step[i]] = current_state[0]\n",
    "                episode_vector_features[i, agent_step[i]] = current_state[1]\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    memories[i].append_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                               episode_vector_features[i, 0: agent_step[i]],\n",
    "                                               episode_rewards[i, 0: agent_step[i]],\n",
    "                                               episode_dones[i, 0: agent_step[i]],\n",
    "                                               episode_actions[i, 0: agent_step[i]],\n",
    "                                               episode_policies[i, 0: agent_step[i]],\n",
    "                                               weight=0)\n",
    "#                     agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "#                                                episode_dones[i], episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "#                 with open(\"./Paipu/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".txt\", 'w+') as fp:\n",
    "#                     fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "#                     break\n",
    "            else:\n",
    "                if np.random.rand() < 0.005: ## no score change\n",
    "                    for i in range(4):\n",
    "                        memories[i].append_episode(episode_matrix_features[i, 0: agent_step[i]],\n",
    "                                                   episode_vector_features[i, 0: agent_step[i]],\n",
    "                                                   episode_rewards[i, 0: agent_step[i]],\n",
    "                                                   episode_dones[i, 0: agent_step[i]],\n",
    "                                                   episode_actions[i, 0: agent_step[i]],\n",
    "                                                   episode_policies[i, 0: agent_step[i]],\n",
    "                                                   weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "            \n",
    "#             st = time.time()\n",
    "#             for n_train in range(5):\n",
    "#                 for i in range(4):\n",
    "#                     agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start, logging=True)\n",
    "#             et = time.time()\n",
    "#             learn2_time += et - st\n",
    "            \n",
    "#             et_done = time.time()\n",
    "#             done_time += et_done - st_done\n",
    "            \n",
    "            et_all = time.time()\n",
    "            all_time += et_all - st_all\n",
    "\n",
    "# data = {\"rons\": env.final_score_changes}\n",
    "# sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories[1].filled_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    memories[i].save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2_RanHoraPolicy_228\"  + \".npz\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.pcolor(env.get_state_(i)[0])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.GameLogToString(env.t.game_log).decode('GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.get_self_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
