{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naiveAI import AgentPER, NMnaive\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import SimpleMahjongBufferPER\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "\n",
    "env = EnvMahjong()\n",
    "\n",
    "agents = [AgentPER(nn=NMnaive(graphs[i], agent_no=i), memory=SimpleMahjongBufferPER(size=1024), greedy=10.0 ** np.random.uniform(-1, 1)) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example \n",
    "# model_path =  \"../log/Agent0-20190501-175203-Game0/naiveAI.ckpt\"\n",
    "# for i in range(4):\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "### This is for AI agents those only cares about itself, i.e., no defense. Therefore, there is no negative reward.\n",
    "\n",
    "### Also, 能和则和，能立直则立直"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_games = 1000000\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    if n % 10000 == 0:\n",
    "        for i in range(4):\n",
    "            agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = [[], [], [], []]\n",
    "    episode_states = [[], [], [], []]\n",
    "    episode_rewards = [[], [], [], []]\n",
    "    \n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "\n",
    "            ######################## 能和则和，能立直则立直 ##############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            for a in range(len(aval_actions)):\n",
    "                if aval_actions[a].action == mp.Action.Riichi:\n",
    "                    good_actions.append(a)\n",
    "                    \n",
    "                if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                    good_actions.append(a)\n",
    "            ##########################################################\n",
    "            \n",
    "            next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            next_aval_states = np.reshape(next_aval_states, [-1, 34, 4, 1])\n",
    "            \n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select(next_aval_states[good_actions])\n",
    "                action = good_actions[a_in_good_as]\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(max(0., r))\n",
    "\n",
    "#             agents[who].learn()\n",
    "\n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "        elif what == \"response\":\n",
    "            next_aval_states_all = []\n",
    "            policies = [[], [], [], []]\n",
    "            for i in range(4):\n",
    "                next_aval_states = env.get_aval_next_states(i)\n",
    "                next_aval_states = np.reshape(next_aval_states, [-1, 34, 4, 1])\n",
    "                next_aval_states_all.append(next_aval_states)\n",
    "                \n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Ron:\n",
    "                        good_actions.append(a)\n",
    "                \n",
    "                    if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                        good_actions.append(a)\n",
    "                ##########################################################\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select(np.reshape(next_aval_states[good_actions], [-1, 34, 4, 1]))\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(np.reshape(next_aval_states, [-1, 34, 4, 1]))\n",
    "                \n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                next_states[i] = env.get_state_(i)\n",
    "                episode_dones[i].append(done)\n",
    "                episode_states[i].append(this_states[i])\n",
    "                episode_rewards[i].append(max(0., rs[i]))\n",
    "#                 agents[i].learn()\n",
    "        \n",
    "            ## next step\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                episode_states[i].append(env.get_state_(i))\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_states[i], episode_rewards[i], episode_dones[i], weight=np.max(final_score_change))\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type)\n",
    "            else:\n",
    "                if np.random.rand() < 0.1: ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_states[i], episode_rewards[i], episode_dones[i], weight=0)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type)\n",
    "                    \n",
    "            for n_train in range(5):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_hand, episode_start=128, care_others=False)\n",
    "            \n",
    "\n",
    "data = {\"rons\": env.rons}\n",
    "sio.savemat(\"./PERrons\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yi qi guan tong\n",
    "hand_matrix_yiqi = \\\n",
    "[[1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] \n",
    "\n",
    "\n",
    "# pi hu\n",
    "hand_matrix_pihu = \\\n",
    "[[0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [1, 1, 1, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]]\n",
    "\n",
    "\n",
    "# lan pai\n",
    "hand_matrix_lan = \\\n",
    "[[1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] \n",
    "\n",
    "\n",
    "\n",
    "# yi qi guan tong 2\n",
    "hand_matrix_yiqi2 = \\\n",
    "[[0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\\\n",
    "\\\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    " [1, 0, 0, 0],\n",
    "\\\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0],\n",
    " [0, 0, 0, 0]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一气贯通m\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_yiqi, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 屁胡\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_pihu, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不可能听牌的情况\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_lan, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一气贯通s\n",
    "print(agents[0].nn.output(np.reshape(hand_matrix_yiqi2, [1, 34, 4, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
