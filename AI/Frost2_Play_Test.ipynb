{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dongqi Han\\AppData\\Local\\conda\\conda\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "from aiFrost2 import AgentFrost2, MahjongNetFrost2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from buffer import MahjongBufferFrost2\n",
    "import MahjongPy as mp\n",
    "from wrapper import EnvMahjong2\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "graphs = [tf.Graph(), tf.Graph(), tf.Graph(), tf.Graph() ]\n",
    "\n",
    "env = EnvMahjong2()\n",
    "\n",
    "num_tile_type = env.matrix_feature_size[0]\n",
    "num_each_tile = env.matrix_feature_size[1]\n",
    "num_vf = env.vector_feature_size\n",
    "\n",
    "agents = [AgentFrost2(nn=MahjongNetFrost2(graphs[i], agent_no=i, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      memory=MahjongBufferFrost2(size=24, num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf),\n",
    "                      greedy=10.0 ** np.random.uniform(-1, 1),\n",
    "                      num_tile_type=num_tile_type, num_each_tile=num_each_tile, num_vf=num_vf)\n",
    "          for i in range(4)]\n",
    "\n",
    "\n",
    "episode_start = 1\n",
    "episode_savebuffer = 128\n",
    "mu_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的对局buffer， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "# for i in range(4):\n",
    "#     buffer_path =  \"./buffer/Agent{}\".format(i) + \"-MahjongBufferFrost220190521-155324.npz\"\n",
    "#     agents[i].memory.load(buffer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  以下的代码可以让Agent读取保存的网络， 如果comment掉就可以让Agent从头开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "# for i in range(4):\n",
    "#     model_path =  \"../log/Agent{}\".foramt(i) + \"-20190501-175203-Game0/naiveAI.ckpt\"\n",
    "#     agents[i].nn.restore(model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      " Game 15"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "append_episode() missing 1 required positional argument: 'mu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a1bfcb15869d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                         agents[i].remember_episode(episode_states[i], episode_rewards[i],\n\u001b[1;32m--> 221\u001b[1;33m                                                    episode_dones[i], episode_policies[i], weight=1)\n\u001b[0m\u001b[0;32m    222\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Codes\\mahjong\\AI\\aiFrost2.py\u001b[0m in \u001b[0;36mremember_episode\u001b[1;34m(self, states, rewards, dones, behavior_policies, weight)\u001b[0m\n\u001b[0;32m    227\u001b[0m                                        \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                                        \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_policies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                                        weight=weight)\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;31m# except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m#     print(\"Episode Length 0! Not recorded!\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: append_episode() missing 1 required positional argument: 'mu'"
     ]
    }
   ],
   "source": [
    "\n",
    "n_games = 1000000\n",
    "\n",
    "\n",
    "process_time = 0\n",
    "learn_time = 0 \n",
    "select_time = 0\n",
    "all_time = 0\n",
    "play_time = 0\n",
    "response_time = 0\n",
    "copy_time = 0\n",
    "done_time = 0\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "for n in range(n_games):\n",
    "    \n",
    "    st_all =  time.time()\n",
    "    \n",
    "#     if n % 10000 == 0:\n",
    "#         for i in range(4):\n",
    "#             agents[i].nn.save(model_dir= \"Agent{}-\".format(i) + datetime_str + \"-Game{}\".format(n))  # save network parameters every 10000 episodes\n",
    "    \n",
    "#     for i in range(4):\n",
    "#         if agents[i].memory.tail % episode_savebuffer == 1:\n",
    "#             agents[i].memory.save(\"./buffer/Agent{}-\".format(i) + \"MahjongBufferFrost2\" + datetime_str + \".npz\")\n",
    "    \n",
    "    print(\"\\r Game {}\".format(n), end='')\n",
    "\n",
    "    episode_dones = [[], [], [], []]\n",
    "    episode_states = [[], [], [], []]\n",
    "    episode_rewards = [[], [], [], []]\n",
    "    episode_policies = [[], [], [], []]\n",
    "    \n",
    "    done = 0\n",
    "#     policies = np.zeros([4,], dtype=np.int32)\n",
    "    actions = np.zeros([4,], dtype=np.int32)\n",
    "    rs = np.zeros([4,], dtype=np.float32)\n",
    "    \n",
    "    this_states = env.reset()  ## for all players\n",
    "    \n",
    "    next_aval_states = deepcopy(this_states)\n",
    "    next_states = [[], [], [], []]\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    while not done and step < 10000:\n",
    "\n",
    "        who, what = env.who_do_what()\n",
    "        \n",
    "        st_play = time.time()\n",
    "        ## make selection\n",
    "        if what == \"play\":\n",
    "            \n",
    "            ######################## Draw a tile #####################\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_draw(playerNo=who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            policy = np.zeros([mu_size,], dtype=np.float32)\n",
    "            policy[0] += 1.\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            \n",
    "            ###################### Play a tile #######################\n",
    "            ###### 能和则和，能立直则立直 ############\n",
    "            aval_actions = env.t.get_self_actions()\n",
    "            good_actions = []\n",
    "            \n",
    "            if agents[who].memory.filled_size < episode_start:  # For collecting data only\n",
    "                for a in range(len(aval_actions)):\n",
    "                    if aval_actions[a].action == mp.Action.Riichi:\n",
    "                        good_actions.append(a)\n",
    "\n",
    "                    if aval_actions[a].action == mp.Action.Tsumo:\n",
    "                        good_actions.append(a)\n",
    "            #######################################\n",
    "            st_process = time.time()\n",
    "\n",
    "            next_aval_states = env.get_aval_next_states(who)  ## for a single player\n",
    "            \n",
    "            et_process = time.time()\n",
    "            process_time += et_process - st_process     \n",
    "            \n",
    "            st = time.time()\n",
    "            if len(good_actions) > 0:\n",
    "                good_actions = np.reshape(good_actions, [-1, ])\n",
    "                a_in_good_as, policy = agents[who].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                           np.array(next_aval_states[1])[good_actions]])\n",
    "                action = good_actions[a_in_good_as]\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[good_actions] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            else:\n",
    "                action, policy = agents[who].select(next_aval_states)\n",
    "                # covert policy to vector (with padding)\n",
    "                tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                tmp[:np.shape(policy)[0]] = policy\n",
    "                policy = deepcopy(tmp)\n",
    "            et = time.time()\n",
    "            select_time += et - st\n",
    "            \n",
    "            next_states[who], r, done, _ = env.step_play(action, playerNo=who)\n",
    "            \n",
    "            next_states[who] = env.get_state_(who)\n",
    "            \n",
    "            episode_dones[who].append(done)\n",
    "            episode_states[who].append(this_states[who])\n",
    "            episode_rewards[who].append(r)\n",
    "            episode_policies[who].append(policy) # only 1 available action (draw)\n",
    "            \n",
    "            this_states[who] = deepcopy(next_states[who])\n",
    "            et_play = time.time()\n",
    "            play_time += et_play - st_play\n",
    "#             step += 2\n",
    "        \n",
    "        st_response = time.time()\n",
    "        if what == \"response\":\n",
    "            policies = [np.zeros([mu_size,], dtype=np.float32) for _ in range(4)]\n",
    "            for i in range(4):\n",
    "                st_process = time.time()\n",
    "                next_aval_states = env.get_aval_next_states(i)\n",
    "                et_process = time.time()\n",
    "                process_time += et_process - st_process\n",
    "                ######################## 能和则和，能立直则立直 ##############\n",
    "                aval_actions = env.t.get_response_actions()\n",
    "                good_actions = []\n",
    "                \n",
    "                if agents[i].memory.filled_size < episode_start:  # For collecting data only\n",
    "                    for a in range(len(aval_actions)):\n",
    "                        if aval_actions[a].action == mp.Action.Ron:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanKan:\n",
    "                            good_actions.append(a)\n",
    "\n",
    "                        if aval_actions[a].action == mp.Action.ChanAnKan:\n",
    "                            good_actions.append(a)\n",
    "                ##########################################################\n",
    "                st = time.time()\n",
    "                if len(good_actions) > 0:\n",
    "                    good_actions = np.reshape(good_actions, [-1, ])\n",
    "                    a_in_good_as, policies[i] = agents[i].select([np.array(next_aval_states[0])[good_actions],\n",
    "                                                                  np.array(next_aval_states[1])[good_actions]])\n",
    "                    actions[i] = good_actions[a_in_good_as]\n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[good_actions] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                    \n",
    "                else:\n",
    "                    actions[i], policies[i] = agents[i].select(next_aval_states)\n",
    "                    \n",
    "                    # covert policy to vector (with padding)\n",
    "                    tmp = np.zeros([mu_size,], dtype=np.float32)\n",
    "                    tmp[:np.shape(policies[i])[0]] = policies[i]\n",
    "                    policies[i] = deepcopy(tmp)\n",
    "                \n",
    "                et = time.time()\n",
    "                select_time += et - st\n",
    "                next_states[i], rs[i], done, _ = env.step_response(actions[i], playerNo=i)\n",
    "                \n",
    "                ## Note: next_states is agent's prediction, but not the true one\n",
    "                \n",
    "            # table change after all players making actions\n",
    "\n",
    "            for i in range(4):\n",
    "                next_states[i] = env.get_state_(i)\n",
    "                episode_dones[i].append(done)\n",
    "                episode_states[i].append(this_states[i])\n",
    "                episode_rewards[i].append(rs[i])\n",
    "                episode_policies[i].append(policies[i]) # only 1 available action (draw)\n",
    "        \n",
    "            ## next step\n",
    "            st_copy = time.time()\n",
    "            for i in range(4):\n",
    "                this_states[i] = deepcopy(next_states[i])\n",
    "            et_copy = time.time()\n",
    "            copy_time += et_copy - st_copy\n",
    "            \n",
    "            step += 1\n",
    "        et_response = time.time()\n",
    "        response_time += et_response - st_response\n",
    "#         print(\"Game {}, step {}\".format(n, step))\n",
    "#         print(env.get_phase_text())\n",
    "        \n",
    "        if done:      \n",
    "            st_done = time.time()\n",
    "            final_score_change = env.get_final_score_change()\n",
    "            for i in range(4):\n",
    "                agents[i].statistics(i, env.t.get_result(), env.get_final_score_change(), env.t.turn,\n",
    "                                     env.t.players[i].riichi, env.t.players[i].menchin)\n",
    "\n",
    "                episode_states[i].append(env.get_state_(i))\n",
    "                \n",
    "                if len(episode_dones[i]) >= 1: # if not 1st turn end\n",
    "                    episode_dones[i][-1] = 1\n",
    "                \n",
    "                #### Disable the following line if not care others\n",
    "#                 episode_rewards[i][-1] = final_score_change[i]\n",
    "                ##################################################\n",
    "            \n",
    "            if not np.max(final_score_change) == 0: ## score change\n",
    "                for i in range(4):\n",
    "                    agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                               episode_dones[i], episode_policies[i], weight=1)\n",
    "                print(' ')\n",
    "                print(env.t.get_result().result_type, end='')\n",
    "                print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "                \n",
    "                with open(\"./Paipu/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\".txt\", 'w+') as fp:\n",
    "                    fp.write(mp.GameLogToString(env.t.game_log).decode('GBK'))\n",
    "            else:\n",
    "                if np.random.rand() < 0.025: ## no score change\n",
    "                    for i in range(4):\n",
    "                        agents[i].remember_episode(episode_states[i], episode_rewards[i],\n",
    "                                                   episode_dones[i], episode_policies[i], weight=1)\n",
    "                    print(' ')\n",
    "                    print(env.t.get_result().result_type, end='')\n",
    "                    print(\": Totally {} steps\".format(np.shape(episode_dones[0])[0]))\n",
    "            \n",
    "            st = time.time()\n",
    "            for n_train in range(5):\n",
    "                for i in range(4):\n",
    "                    agents[i].learn(env.symmetric_matrix_features, episode_start=episode_start, logging=False)\n",
    "            et = time.time()\n",
    "            learn_time += et - st\n",
    "            \n",
    "            et_done = time.time()\n",
    "            done_time += et_done - st_done\n",
    "            \n",
    "            et_all = time.time()\n",
    "            all_time += et_all - st_all\n",
    "\n",
    "data = {\"rons\": env.final_score_changes}\n",
    "sio.savemat(\"./final_score_changes\" + datetime_str + \".mat\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.get_response_actions()[0].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_time)\n",
    "print(select_time)\n",
    "print(learn_time)\n",
    "print(play_time)\n",
    "print(response_time)\n",
    "print(process_time)\n",
    "print(done_time)\n",
    "print(copy_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check tiles\n",
    "for p in range(4):\n",
    "    hand = env.t.players[p].hand\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(hand)):\n",
    "        print(hand[k].tile)\n",
    "for p in range(4):\n",
    "    fulus = env.t.players[p].fulus\n",
    "    print('player {}'.format(p))\n",
    "    for k in range(len(fulus)):\n",
    "        print(fulus[k].to_string())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.t.DORA[0].tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_states[0][0][:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(1):\n",
    "    plt.pcolor(env.get_state_(i)[0][:,:5])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.pcolor(env.get_next_state(0, i)[0])\n",
    "    print(env.t.get_response_actions()[0].action)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict score (value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MahjongPy as mp\n",
    "t = mp.Table()\n",
    "t.game_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = t.get_next_aval_states_matrix_features_frost2(0)[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.GameLogToString(env.t.game_log).decode('GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin([-1, -1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22f97f82f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(np.reshape(np.array(S), [34, 58]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparse as sps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'coords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6fb240669cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m34\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m58\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-6fb240669cf3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m34\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m58\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'coords'"
     ]
    }
   ],
   "source": [
    "S = [sps.COO(shape=(200, 20, 34, 58)) for _ in range(1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[1].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
